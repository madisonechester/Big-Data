{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31682f9a-4945-4feb-bc89-83340720aa3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "#### First Name: Madison \n",
    "#### Last Name: Chester\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a93105-2c23-4b4e-9362-e9c2aea7f293",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Load Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d994cc31-0b26-4848-9f4a-01b12ca47e96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8dbea9-daf7-4134-b06d-9c414621621b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_twitter = spark.read.json(\"/FileStore/tables/corona_tweet_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32afcf7e-c82b-4c27-91e3-028eb7f11302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- favorite_count: long (nullable = true)\n |-- hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- in_reply_to_status_id: string (nullable = true)\n |-- in_reply_to_user_id_str: string (nullable = true)\n |-- location: string (nullable = true)\n |-- reply_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- source: string (nullable = true)\n |-- user: struct (nullable = true)\n |    |-- contributors_enabled: boolean (nullable = true)\n |    |-- created_at: string (nullable = true)\n |    |-- default_profile: boolean (nullable = true)\n |    |-- default_profile_image: boolean (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- favourites_count: long (nullable = true)\n |    |-- follow_request_sent: string (nullable = true)\n |    |-- followers_count: long (nullable = true)\n |    |-- following: string (nullable = true)\n |    |-- friends_count: long (nullable = true)\n |    |-- geo_enabled: boolean (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- id_str: string (nullable = true)\n |    |-- is_translator: boolean (nullable = true)\n |    |-- lang: string (nullable = true)\n |    |-- listed_count: long (nullable = true)\n |    |-- location: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- notifications: string (nullable = true)\n |    |-- profile_background_color: string (nullable = true)\n |    |-- profile_background_image_url: string (nullable = true)\n |    |-- profile_background_image_url_https: string (nullable = true)\n |    |-- profile_background_tile: boolean (nullable = true)\n |    |-- profile_banner_url: string (nullable = true)\n |    |-- profile_image_url: string (nullable = true)\n |    |-- profile_image_url_https: string (nullable = true)\n |    |-- profile_link_color: string (nullable = true)\n |    |-- profile_sidebar_border_color: string (nullable = true)\n |    |-- profile_sidebar_fill_color: string (nullable = true)\n |    |-- profile_text_color: string (nullable = true)\n |    |-- profile_use_background_image: boolean (nullable = true)\n |    |-- protected: boolean (nullable = true)\n |    |-- screen_name: string (nullable = true)\n |    |-- statuses_count: long (nullable = true)\n |    |-- time_zone: string (nullable = true)\n |    |-- translator_type: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- utc_offset: string (nullable = true)\n |    |-- verified: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2460671-b6a6-47bd-84ba-e63c00bd541a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### from the user nestec col, select the following cols only: id_str, followers_count, friends_count and created_at (2 points)\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "df_twitter = df_twitter.select(\n",
    "  col('source'),\n",
    "  col('hashtags'),\n",
    "  col('retweet_count'),\n",
    "  col('id'),\n",
    "  col('location'),\n",
    "  col('user.id_str').alias('user_id_str'),  # select 'id_str' and change the column name\n",
    "  col('user.followers_count').alias('user_followers_count'),  # select 'followers_count' and change the column name\n",
    "  col('user.friends_count').alias('user_friends_count'),  # select 'friends_count' and change the column name\n",
    "  col('user.created_at').alias('user_created_at')  # select 'created_at' and change the column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5404794-ac4a-40d0-920d-15415f23f764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 15894"
     ]
    }
   ],
   "source": [
    "### print the total count of number of records in df_twitter (1 point)\n",
    "df_twitter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b937f2f-1ee5-4f6b-82fc-dd26c80c1318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n|   extracted_source|              source|\n+-------------------+--------------------+\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter Web Client|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n+-------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "### extract the source lable from source col by droping the anchor tab, and save it as another col named extracted_source (4 points)\n",
    "# for example <a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a> => Twitter Web App\n",
    "# you can use \"<a [^>]+>([^<]+)\" as the regualr expresion, and the group would be 1 for this regular expression\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# apply regexp_extract to extract the label from the 'source' column\n",
    "df_twitter = df_twitter.withColumn('extracted_source', regexp_extract(col('source'), '<a[^>]*>(.*?)</a>', 1))\n",
    "df_twitter.select(col('extracted_source'), col('source')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc2a2ab-95df-4a32-afab-bc1519176136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert the DataFrame into RDD\n",
    "rdd_twitter = df_twitter.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe54b15-11ee-451e-ad62-188a3537fe85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### create a temporary table in memory with the name as twitter (1 point)\n",
    "df_twitter.createOrReplaceTempView('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3bf085-81dd-4900-b4da-d8992988eef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Analyze Data\n",
    "\n",
    "#### You will be writing code to find the answer to the questions listed below using RDD/spark SQL.\n",
    "\n",
    "- Analyze using RDD \n",
    "- Analyze using Dataframe without temporary table \n",
    "- Analyze using spark.sql with temporary table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3718e4f-b9bb-47d4-bcdb-1b95060ea597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Get total number of unique users (1 point each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3a7e59-4e9c-4bcd-83c0-95b20af7409f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14094\n"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "user_id_index = df_twitter.columns.index('user_id_str')\n",
    "user_id_rdd = rdd_twitter.map(lambda row: row[user_id_index])\n",
    "unique_users = user_id_rdd.distinct().count()\n",
    "print(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8811c4f1-b7e9-402c-b173-d911d3fbd30b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: 14094"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "df_twitter.select(col('user_id_str')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166afb53-4bf9-4188-8e34-4d6860be63d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 14094"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "spark.sql('select distinct user_id_str from twitter').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026629d1-5215-40c7-a274-33514f261eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Get count of users who have more than 1 tweet in the data (2 points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2088c395-da3a-409a-8c94-8fa678d3593c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 1016"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "user_id_index = df_twitter.columns.index('user_id_str')\n",
    "user_id_rdd = rdd_twitter.map(lambda row: row[user_id_index])\n",
    "user_count_rdd = user_id_rdd.map(lambda user_id: (user_id, 1)).reduceByKey(lambda x, y: x + y)\n",
    "user_count_rdd.filter(lambda x: x[1] > 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71465c1d-d375-4052-adbe-e95dd671b955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 1016"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "df_twitter.groupBy('user_id_str').count().filter(col('count') > 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5e04a7-13da-4c8f-b2fc-6af9a3791722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 1016"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "query = \"\"\"\n",
    "    SELECT user_id_str, COUNT(*) AS count\n",
    "    FROM twitter\n",
    "    GROUP BY user_id_str\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\"\n",
    "spark.sql(query).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6d8f82-ab2b-4bc9-b266-b2486624f2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Get total number unique extracted_source (1 point each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcdc745-55f2-4e9c-84d7-267086ba437c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: 133"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "extracted_source_index = df_twitter.columns.index('extracted_source')\n",
    "extracted_source_rdd = rdd_twitter.map(lambda row: row[extracted_source_index])\n",
    "extracted_source_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9974fd-f96e-4b8c-97c8-fcbae505dd91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: 133"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "df_twitter.select(col('extracted_source')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f436e2ae-19ba-46ba-825c-18d97c5f30bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 133"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "query = f\"select distinct {'extracted_source'} from twitter\"\n",
    "spark.sql(query).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188bdf59-cf92-4d08-927b-f003e0098ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Get top 5 most used extracted_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07037b6-c226-4de2-b942-da6f732b20ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [('Twitter for Android', 6262),\n ('Twitter for iPhone', 5698),\n ('Twitter Web App', 2878),\n ('Twitter for iPad', 428),\n ('Twitter Web Client', 136)]"
     ]
    }
   ],
   "source": [
    "# using RDD (5 points)\n",
    "extracted_source_index = df_twitter.columns.index('extracted_source')\n",
    "extracted_source_rdd = rdd_twitter.map(lambda row: row[extracted_source_index])\n",
    "extracted_source_count_rdd = extracted_source_rdd.map(lambda extracted_source: (extracted_source, 1)).reduceByKey(lambda x, y: x + y)\n",
    "extracted_source_count_rdd \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8175f756-d354-46b9-8ecb-24798c606293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|   extracted_source|count|\n+-------------------+-----+\n|Twitter for Android| 6262|\n| Twitter for iPhone| 5698|\n|    Twitter Web App| 2878|\n|   Twitter for iPad|  428|\n| Twitter Web Client|  136|\n+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using DataFrame (2 points)\n",
    "df_twitter.groupBy('extracted_source').count().orderBy(col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff8fab4-c6b3-4dec-ba2e-525ef27eb304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|   extracted_source|count|\n+-------------------+-----+\n|Twitter for Android| 6262|\n| Twitter for iPhone| 5698|\n|    Twitter Web App| 2878|\n|   Twitter for iPad|  428|\n| Twitter Web Client|  136|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table (2 points)\n",
    "query = \"\"\"\n",
    "    SELECT extracted_source, COUNT(*) AS count\n",
    "    FROM twitter\n",
    "    GROUP BY extracted_source\n",
    "    ORDER BY count DESC\n",
    "    limit 5\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b302336-03bd-448f-9608-a63b4ef1daa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Get count of distinct hastags used (5 points each) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945573b7-aaa7-44d0-8086-4230a8f81d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: 1215"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "# need to flatmap the result since hastags is a nested column\n",
    "hashtags_index = df_twitter.columns.index('hashtags')\n",
    "hashtags_rdd = rdd_twitter.flatMap(lambda row: row[hashtags_index])\n",
    "hashtags_rdd.filter(lambda hashtags: hashtags is not None and len(hashtags) > 0).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34af0927-8b11-4c2c-b653-aabc3b5de016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: 1215"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "hashtags = df_twitter.select(explode(col('hashtags')).alias('hashtag'))\n",
    "hashtags.filter(col('hashtag') != \"\").select(col('hashtag')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de8afaa-e618-46b7-a471-786267e8da6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW hashtags_explode AS\n",
    "    SELECT explode(hashtags) AS hashtags\n",
    "    FROM twitter\n",
    "\"\"\")\n",
    "\n",
    "hashtags_unique_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT hashtags) AS hashtags_unique_count\n",
    "    FROM hashtags_explode\n",
    "    WHERE hashtags != ''\n",
    "\"\"\").collect()[0][0]\n",
    "\n",
    "print(hashtags_unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedd1db9-f6e1-4114-8187-76e447dd568b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Get top 5 hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae2407d-566e-4071-b2be-b85930690339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [('طبق_القدرات_للثانويه_ياريس', 385),\n ('Corona', 319),\n ('OilPrice', 251),\n ('COVID19', 125),\n ('corona', 123)]"
     ]
    }
   ],
   "source": [
    "# using RDD (4 points)\n",
    "hashtags_rdd.map(lambda hashtags: (hashtags, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739048d3-575b-49ed-a5bc-f92cd30ba224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|             hashtag|count|\n+--------------------+-----+\n|طبق_القدرات_للثان...|  385|\n|              Corona|  319|\n|            OilPrice|  251|\n|             COVID19|  125|\n|              corona|  123|\n+--------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using DataFrame (2 points)\n",
    "hashtags.groupBy('hashtag').count().orderBy(col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8357166-5055-4127-a832-1df5ac844808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|            hashtags|count|\n+--------------------+-----+\n|طبق_القدرات_للثان...|  385|\n|              Corona|  319|\n|            OilPrice|  251|\n|             COVID19|  125|\n|              corona|  123|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table (2 points)\n",
    "top_hashtags = spark.sql(\"\"\"\n",
    "    SELECT hashtags, COUNT(*) AS count\n",
    "    FROM hashtags_explode\n",
    "    WHERE hashtags != ''\n",
    "    GROUP BY hashtags\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2e235a-34d7-4c48-947b-c092387a21ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.7 Get total number of tweets which are retweeted more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccc01a7-f5ea-48bd-9bac-8b5dfe7fba0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: 15753"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "retweet_count_index = df_twitter.columns.index('retweet_count')\n",
    "rdd_twitter.filter(lambda row: row[retweet_count_index] > 100).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9502388c-f59a-4dc1-82a1-255cab5af332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: 15753"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "df_twitter.filter(col('retweet_count') > 100).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be34af63-a4e5-40ae-8f2e-dae437a0f1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: 15753"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "query = \"\"\"\n",
    "  SELECT * \n",
    "  FROM twitter\n",
    "  WHERE retweet_count > 100\n",
    "\"\"\"\n",
    "spark.sql(query).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48189fee-024e-4ff5-8109-da51120e3453",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.8 Get top 3 most retweeted tweets per country (8 points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b894a602-d6ee-44b5-9c4c-96c92ccf8309",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [('India',\n  [('1252332114948874240', 9988),\n   ('1252252336921206787', 9976),\n   ('1252254519116746754', 9973)]),\n ('Pakistan',\n  [('1252334264248606720', 9988),\n   ('1252251912084357121', 9975),\n   ('1252252126694309888', 9973)]),\n ('USA',\n  [('1252331777806524416', 9994),\n   ('1252254239805579264', 9987),\n   ('1252335464750735362', 9982)]),\n ('Italy',\n  [('1252252106750377996', 9994),\n   ('1252251206027816960', 9984),\n   ('1252330500670337024', 9971)]),\n ('Canada',\n  [('1252335430323888128', 9997),\n   ('1252254877939531776', 9992),\n   ('1252252082825986051', 9987)])]"
     ]
    }
   ],
   "source": [
    "# using RDD\n",
    "location_index = df_twitter.columns.index('location')\n",
    "id_index = df_twitter.columns.index('id')\n",
    "retweet_count_index = df_twitter.columns.index('retweet_count')\n",
    "location_rdd = rdd_twitter.map(lambda row: (row[location_index], (row[id_index], row[retweet_count_index]))).groupByKey()\n",
    "location_sorted_descending = location_rdd.mapValues(lambda tweet: sorted(tweet, key=lambda x: x[1], reverse=True)[:3])\n",
    "location_sorted_descending.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dd149d-2c03-459f-b281-4488db7241bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+\n|location|id                 |retweet_count|\n+--------+-------------------+-------------+\n|Canada  |1252335430323888128|9997         |\n|Canada  |1252254877939531776|9992         |\n|Canada  |1252252082825986051|9987         |\n|Chile   |1252253612140490759|9988         |\n|Chile   |1252334891951427585|9984         |\n|Chile   |1252253710182481920|9978         |\n|China   |1252335780707684352|9998         |\n|China   |1252253596516843520|9993         |\n|China   |1252255562525560832|9984         |\n|Germany |1252334028092399622|9999         |\n|Germany |1252330902325248000|9997         |\n|Germany |1252252295510855682|9990         |\n|India   |1252332114948874240|9988         |\n|India   |1252252336921206787|9976         |\n|India   |1252254519116746754|9973         |\n|Italy   |1252252106750377996|9994         |\n|Italy   |1252251206027816960|9984         |\n|Italy   |1252330500670337024|9971         |\n|Mexico  |1252253843145912320|9998         |\n|Mexico  |1252255209776189442|9994         |\n+--------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using DataFrame\n",
    "from pyspark.sql.functions import col, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# window to partition and order by retweet_count in each location\n",
    "window = Window.partitionBy('location').orderBy(desc('retweet_count'))\n",
    "# row number column to rank tweets in each location by retweet_count\n",
    "ranked = df_twitter.withColumn('rank', row_number().over(window))\n",
    "# filter for top 3 most retweeted tweets per country\n",
    "top_tweets = ranked.filter(col('rank') <= 3)\n",
    "# show top 3 most retweeted tweets per country\n",
    "top_tweets.select(col('location'), col('id'), col('retweet_count')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26dc93-f30e-46c5-b959-dc284fc0dbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+\n|location|                 id|retweet_count|\n+--------+-------------------+-------------+\n|  Canada|1252335430323888128|         9997|\n|  Canada|1252254877939531776|         9992|\n|  Canada|1252252082825986051|         9987|\n|   Chile|1252253612140490759|         9988|\n|   Chile|1252334891951427585|         9984|\n|   Chile|1252253710182481920|         9978|\n|   China|1252335780707684352|         9998|\n|   China|1252253596516843520|         9993|\n|   China|1252255562525560832|         9984|\n| Germany|1252334028092399622|         9999|\n| Germany|1252330902325248000|         9997|\n| Germany|1252252295510855682|         9990|\n|   India|1252332114948874240|         9988|\n|   India|1252252336921206787|         9976|\n|   India|1252254519116746754|         9973|\n|   Italy|1252252106750377996|         9994|\n|   Italy|1252251206027816960|         9984|\n|   Italy|1252330500670337024|         9971|\n|  Mexico|1252253843145912320|         9998|\n|  Mexico|1252255209776189442|         9994|\n+--------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY location ORDER BY retweet_count DESC) AS rank\n",
    "        FROM twitter\n",
    "    ) ranked\n",
    "    WHERE rank <= 3\n",
    "\"\"\"\n",
    "spark.sql(query).select(col('location'), col('id'), col('retweet_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29ffc84-4cf0-4588-90dc-84fc373c1855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.9 Total number of tweets per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74d26ec-8348-40e2-948d-5b89ae4df1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [('India', 1480),\n ('Pakistan', 1470),\n ('USA', 1539),\n ('Italy', 1422),\n ('Canada', 1441),\n ('China', 1457),\n ('Chile', 1410),\n ('UK', 1376),\n ('Mexico', 1409),\n ('Spain', 1464),\n ('Germany', 1426)]"
     ]
    }
   ],
   "source": [
    "# using RDD (3 points)\n",
    "rdd_twitter.map(lambda row: row[location_index]).map(lambda location: (location, 1)).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672a3e28-433e-4669-9612-2931ee30563c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|location|count|\n+--------+-----+\n| Germany| 1426|\n|   India| 1480|\n|   China| 1457|\n|   Chile| 1410|\n|   Italy| 1422|\n|   Spain| 1464|\n|     USA| 1539|\n|  Mexico| 1409|\n|      UK| 1376|\n|  Canada| 1441|\n|Pakistan| 1470|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# using DataFrame (2 points)\n",
    "df_twitter.groupby(col('location')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24294129-3f68-4d94-b6e1-627f0f23973d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|location|count|\n+--------+-----+\n|     USA| 1539|\n|   India| 1480|\n|Pakistan| 1470|\n|   Spain| 1464|\n|   China| 1457|\n|  Canada| 1441|\n| Germany| 1426|\n|   Italy| 1422|\n|   Chile| 1410|\n|  Mexico| 1409|\n|      UK| 1376|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql and the temporary table (1 point)\n",
    "query = \"\"\"\n",
    "    SELECT location AS location, COUNT(*) AS count\n",
    "    FROM twitter\n",
    "    WHERE location IS NOT NULL\n",
    "    GROUP BY location\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8907e096-9205-47b6-b76a-513f08a15b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a90e738-eea4-47d2-b029-629ac1d14f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 Save the data such that you have seperate folder per country (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3413113-fbdf-477a-8473-5dad5cc94a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using DataFrame\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "output_path = \"/FileStore/tables/\" \n",
    "locations = df_twitter.select('location').distinct().collect()\n",
    "\n",
    "for location in locations:\n",
    "    country = location['location']\n",
    "    df_country = df_twitter.filter(col('location') == country)\n",
    "    # convert hashtags array to a comma-separated string\n",
    "    df_country = df_country.withColumn('hashtags', concat_ws(',', col('hashtags')))\n",
    "    output_directory = f\"{output_path}/{country.replace(' ', '_')}\"  # replace spaces with underscores for folder names\n",
    "    df_country.write.mode('overwrite').csv(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f27209-6a78-4c31-b50a-cfc91ceea32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Save the data as parquet files (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e65d90-76a0-4fd8-82ff-58f7258fd49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using DataFrame\n",
    "output_directory = \"/FileStore/tables/\" \n",
    "df_twitter.write.partitionBy('location').mode('overwrite').parquet(output_directory)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MadisonChester_Assignment2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
