{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a5ed28-4d02-4e4f-94a2-7005b42a8ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a model to predict the flight delay over 15 minutes (ARR_DEL15) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50882a87-b1f6-4b92-9eec-11a4075743d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark ML Class Assisgnment\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bf3b42-b137-4b36-86e4-7fc856051709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the file\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType\n",
    "df = (spark.read.format(\"csv\").\n",
    "  option(\"header\", \"true\").\n",
    "  option(\"nullValue\", \"NA\").\n",
    "  option(\"inferSchema\", True).\n",
    "  load(\"/FileStore/tables/flight_weather_small.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ba650c-08bd-4687-91af-b6928a995b74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this dataset,\n",
    "\n",
    "ARR_DEL15 : 1 when the flight is delayed over 15 minutes, 0 otherwise.\n",
    "XXXOrigin : Weather conditions in departure airport.\n",
    "XXXDest : Weather conditions in destination airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a35fb3c-f41d-4115-83d5-c9296289bf8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- X.1: integer (nullable = true)\n |-- YEAR: integer (nullable = true)\n |-- MONTH: integer (nullable = true)\n |-- DAY_OF_MONTH: integer (nullable = true)\n |-- DAY_OF_WEEK: integer (nullable = true)\n |-- FL_DATE: date (nullable = true)\n |-- UNIQUE_CARRIER: string (nullable = true)\n |-- TAIL_NUM: string (nullable = true)\n |-- FL_NUM: integer (nullable = true)\n |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_STATE_ABR: string (nullable = true)\n |-- DEST_AIRPORT_ID: integer (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_STATE_ABR: string (nullable = true)\n |-- CRS_DEP_TIME: integer (nullable = true)\n |-- DEP_TIME: integer (nullable = true)\n |-- DEP_DELAY: integer (nullable = true)\n |-- DEP_DELAY_NEW: integer (nullable = true)\n |-- DEP_DEL15: integer (nullable = true)\n |-- DEP_DELAY_GROUP: integer (nullable = true)\n |-- TAXI_OUT: integer (nullable = true)\n |-- WHEELS_OFF: integer (nullable = true)\n |-- WHEELS_ON: integer (nullable = true)\n |-- TAXI_IN: integer (nullable = true)\n |-- CRS_ARR_TIME: integer (nullable = true)\n |-- ARR_TIME: integer (nullable = true)\n |-- ARR_DELAY: integer (nullable = true)\n |-- ARR_DELAY_NEW: integer (nullable = true)\n |-- ARR_DEL15: integer (nullable = true)\n |-- ARR_DELAY_GROUP: integer (nullable = true)\n |-- CANCELLED: integer (nullable = true)\n |-- CANCELLATION_CODE: string (nullable = true)\n |-- DIVERTED: integer (nullable = true)\n |-- CRS_ELAPSED_TIME: integer (nullable = true)\n |-- ACTUAL_ELAPSED_TIME: integer (nullable = true)\n |-- AIR_TIME: integer (nullable = true)\n |-- FLIGHTS: integer (nullable = true)\n |-- DISTANCE: integer (nullable = true)\n |-- DISTANCE_GROUP: integer (nullable = true)\n |-- CARRIER_DELAY: integer (nullable = true)\n |-- WEATHER_DELAY: integer (nullable = true)\n |-- NAS_DELAY: integer (nullable = true)\n |-- SECURITY_DELAY: integer (nullable = true)\n |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n |-- X: string (nullable = true)\n |-- VisibilityOrigin: double (nullable = true)\n |-- DryBulbCelsiusOrigin: double (nullable = true)\n |-- DewPointCelsiusOrigin: double (nullable = true)\n |-- RelativeHumidityOrigin: double (nullable = true)\n |-- WindSpeedOrigin: double (nullable = true)\n |-- AltimeterOrigin: double (nullable = true)\n |-- VisibilityDest: double (nullable = true)\n |-- DryBulbCelsiusDest: double (nullable = true)\n |-- DewPointCelsiusDest: double (nullable = true)\n |-- RelativeHumidityDest: double (nullable = true)\n |-- WindSpeedDest: double (nullable = true)\n |-- AltimeterDest: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# check sample data from table\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d347f06e-9a3f-4192-a544-7bc84a2c6986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mark as \"delayed over 15 minutes\" if it's canceled\n",
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn(\"ARR_DEL15\", when(df[\"CANCELLED\"] == 1, 1).otherwise(df[\"ARR_DEL15\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bf65d9-e168-445e-b5d4-6a6739fe5866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove flights if it's diverted\n",
    "df = df.filter(df[\"DIVERTED\"] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f133dba8-a6ca-4de8-9d75-8f836f82c745",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Narrow to required columns.\n",
    "\n",
    "\"ARR_DEL15\",\n",
    "  \"MONTH\",\n",
    "  \"DAY_OF_WEEK\",\n",
    "  \"UNIQUE_CARRIER\",\n",
    "  \"ORIGIN\",\n",
    "  \"DEST\",\n",
    "  \"CRS_DEP_TIME\",\n",
    "  \"CRS_ARR_TIME\",\n",
    "  \"RelativeHumidityOrigin\",\n",
    "  \"AltimeterOrigin\",\n",
    "  \"DryBulbCelsiusOrigin\",\n",
    "  \"WindSpeedOrigin\",\n",
    "  \"VisibilityOrigin\",\n",
    "  \"DewPointCelsiusOrigin\",\n",
    "  \"RelativeHumidityDest\",\n",
    "  \"AltimeterDest\",\n",
    "  \"DryBulbCelsiusDest\",\n",
    "  \"WindSpeedDest\",\n",
    "  \"VisibilityDest\",\n",
    "  \"DewPointCelsiusDest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518a304f-e12a-4c04-95b2-842837be1fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select required columns\n",
    "required_columns = [\n",
    "    \"ARR_DEL15\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"UNIQUE_CARRIER\",\n",
    "    \"ORIGIN\",\n",
    "    \"DEST\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"RelativeHumidityOrigin\",\n",
    "    \"AltimeterOrigin\",\n",
    "    \"DryBulbCelsiusOrigin\",\n",
    "    \"WindSpeedOrigin\",\n",
    "    \"VisibilityOrigin\",\n",
    "    \"DewPointCelsiusOrigin\",\n",
    "    \"RelativeHumidityDest\",\n",
    "    \"AltimeterDest\",\n",
    "    \"DryBulbCelsiusDest\",\n",
    "    \"WindSpeedDest\",\n",
    "    \"VisibilityDest\",\n",
    "    \"DewPointCelsiusDest\"\n",
    "]\n",
    "\n",
    "df = df.select(required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0525d8-f660-46f9-86c3-72a6fa25a17b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop rows with null value\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676bb3db-42c3-41c2-a1b2-f41001144339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split data into training data and evaluation data (ratio 80% : 20%)\n",
    "train_data, eval_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bc389a-c9c7-477e-a4a9-0672dbffa6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convert categorical values to index values (0, 1, ...) for the following columns.\n",
    "\n",
    "Carrier code (UNIQUE_CARRIER)\n",
    "Airport code in departure (ORIGIN)\n",
    "Airport code in destination (DEST)\n",
    "Flag (0 or 1) for delay over 15 minutes (ARR_DEL15)\n",
    "\n",
    "hint: pyspark.ml.feature check StringIndexer transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65b5071-41a5-4890-9fb5-67cbe23ced49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+------------+----------------------+---------------+--------------------+---------------+----------------+---------------------+--------------------+-------------+------------------+-------------+--------------+-------------------+----------------------+--------------+------------+---------+\n|MONTH|DAY_OF_WEEK|CRS_DEP_TIME|CRS_ARR_TIME|RelativeHumidityOrigin|AltimeterOrigin|DryBulbCelsiusOrigin|WindSpeedOrigin|VisibilityOrigin|DewPointCelsiusOrigin|RelativeHumidityDest|AltimeterDest|DryBulbCelsiusDest|WindSpeedDest|VisibilityDest|DewPointCelsiusDest|Indexed_UNIQUE_CARRIER|Indexed_ORIGIN|Indexed_DEST|ARR_DEL15|\n+-----+-----------+------------+------------+----------------------+---------------+--------------------+---------------+----------------+---------------------+--------------------+-------------+------------------+-------------+--------------+-------------------+----------------------+--------------+------------+---------+\n|    1|          3|           9|          12|                  29.0|           30.1|                -3.9|            7.0|            10.0|                -19.4|                65.0|        30.18|              17.2|          3.0|          10.0|               10.6|                   2.0|          14.0|         4.0|      0.0|\n|    1|          5|           9|          12|                  28.0|          30.21|                 0.0|           15.0|            10.0|                -16.7|                78.0|        30.09|              13.9|          6.0|          10.0|               10.0|                   2.0|          14.0|         4.0|      0.0|\n|    1|          4|           9|          18|                  49.0|          30.02|                18.3|            8.0|            10.0|                  7.2|                72.0|        29.99|              -0.6|          9.0|          10.0|               -5.0|                   2.0|           4.0|        16.0|      0.0|\n|    1|          6|           9|          18|                  56.0|          29.99|                15.6|           18.0|            10.0|                  6.7|                69.0|        30.28|              -3.3|         13.0|          10.0|               -8.3|                   2.0|           4.0|        16.0|      1.0|\n|    1|          5|          12|          15|                  54.0|          29.78|                 8.9|           13.0|            10.0|                  0.0|                90.0|        29.99|              13.3|          0.0|           6.0|               11.7|                   2.0|          14.0|         4.0|      0.0|\n+-----+-----------+------------+------------+----------------------+---------------+--------------------+---------------+----------------+---------------------+--------------------+-------------+------------------+-------------+--------------+-------------------+----------------------+--------------+------------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# convert categorical values to index values\n",
    "uniqueCarrierIndexer = StringIndexer(inputCol=\"UNIQUE_CARRIER\", outputCol=\"Indexed_UNIQUE_CARRIER\").fit(df)\n",
    "originIndexer = StringIndexer(inputCol=\"ORIGIN\", outputCol=\"Indexed_ORIGIN\").fit(df)\n",
    "destIndexer = StringIndexer(inputCol=\"DEST\", outputCol=\"Indexed_DEST\").fit(df)\n",
    "arrDel15Indexer = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"Indexed_ARR_DEL15\").fit(df)\n",
    "\n",
    "# apply the indexers to the df\n",
    "df = uniqueCarrierIndexer.transform(df)\n",
    "df = originIndexer.transform(df)\n",
    "df = destIndexer.transform(df)\n",
    "df = arrDel15Indexer.transform(df)\n",
    "\n",
    "# drop the original categorical columns as they are now indexed\n",
    "df = df.drop(\"UNIQUE_CARRIER\", \"ORIGIN\", \"DEST\", \"ARR_DEL15\")\n",
    "\n",
    "# rename the indexed ARR_DEL15 column back to ARR_DEL15\n",
    "df = df.withColumnRenamed(\"Indexed_ARR_DEL15\", \"ARR_DEL15\")\n",
    "\n",
    "# show the transformed df\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036ce7b0-e818-41ad-9fe3-9dbbf471fd25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### In Spark machine learning, the feature columns must be wrapped as a single vector value.\n",
    "\n",
    "So create new vector column named \"features\".\n",
    "\n",
    "Hint: pyspark.ml.feature check VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95928755-6133-4040-8b3a-0c2bcf43938b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"Indexed_UNIQUE_CARRIER\",\n",
    "    \"Indexed_ORIGIN\",\n",
    "    \"Indexed_DEST\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"RelativeHumidityOrigin\",\n",
    "    \"AltimeterOrigin\",\n",
    "    \"DryBulbCelsiusOrigin\",\n",
    "    \"WindSpeedOrigin\",\n",
    "    \"VisibilityOrigin\",\n",
    "    \"DewPointCelsiusOrigin\",\n",
    "    \"RelativeHumidityDest\",\n",
    "    \"AltimeterDest\",\n",
    "    \"DryBulbCelsiusDest\",\n",
    "    \"WindSpeedDest\",\n",
    "    \"VisibilityDest\",\n",
    "    \"DewPointCelsiusDest\"],\n",
    "  outputCol = \"features\")\n",
    "\n",
    "# transform the df to include the features column\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4987e429-7a73-41ab-8907-1d5f8c406d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generate classifier. Here we use Decision Tree classifier.\n",
    "\n",
    "Hint: From pyspark.ml.classification check DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c4e936-c366-47f9-86c0-e84da192fa28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# create the Decision Tree classifier\n",
    "classifier = DecisionTreeClassifier(labelCol=\"ARR_DEL15\", featuresCol=\"features\", maxBins=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac0eb2c-d21d-4959-a5ed-ef4545ca8e84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generate SparkML pipeline and run training.\n",
    "Trained model (with coefficients) and pipeline are stored in the variable \"model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce50ae9-53c5-4c41-ae89-d0f96a59a165",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[uniqueCarrierIndexer, originIndexer, destIndexer, arrDel15Indexer, assembler, classifier])\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519fb09b-0677-4772-a237-1c39c7bc91f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Predict with eveluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9ca94b-9f94-4fde-9996-8432bee9bf22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict with evaluation data\n",
    "pred = model.transform(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e7a41c-97cc-4a86-ba97-6f69d814895e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Show eveluation result.\n",
    "\n",
    "Hint: pyspark.ml.evaluation check MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8127b2-26cc-4156-876d-9cf2cce8d6f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation set accuracy = 0.44\n"
     ]
    }
   ],
   "source": [
    "# evaluate results\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Indexed_ARR_DEL15\")\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "print(f\"Evaluation set accuracy = {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1526149f-e15e-425e-8abe-2eec6d8e8127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save pipeline\n",
    "#model.save(\"...\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML-ClassAssisgnment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
